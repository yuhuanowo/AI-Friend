from langchain.agents import ConversationalChatAgent, AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain_community.callbacks import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.runnables import RunnableConfig
from langchain_community.chat_models import ChatOllama
#from transformers import pipeline
import requests
import scipy
import time

#ChatTTS
import ChatTTS
import torch
import soundfile

import base64

import streamlit as st

global EnableMemory #æ˜¯å¦å•Ÿç”¨è¨˜æ†¶
global AudioCount #èªéŸ³è¨ˆæ•¸
AudioCount = 0
autoplay = True
already_generated = True

def process_with_api(sentence):
    # è¿™é‡Œå‡è®¾æœ‰ä¸€ä¸ªå‡½æ•° `process_with_api` ç”¨äºè°ƒç”¨ API å¤„ç†å¥å­
    return sentence


def autoplay_audio(file, autoplay=True, file_type='wav'):
    b64 = base64.b64encode(file).decode()
    if autoplay:
        md = f"""
            <audio id="audioTag" controls autoplay>
            <source src="data:audio/{file_type};base64,{b64}"  type="audio/{file_type}" format="audio/{file_type}">
            </audio>
            """
    else:
        md = f"""
            <audio id="audioTag" controls>
            <source src="data:audio/{file_type};base64,{b64}"  type="audio/{file_type}" format="audio/{file_type}">
            </audio>
            """
    st.markdown(
        md,
        unsafe_allow_html=True,
    )
#================================================================================================
# Streamlit App
#================================================================================================

#streamlit æ¨™é¡Œèˆ‡åœ–æ¨™
st.set_page_config(page_title="AI-Friend", page_icon="ğŸ¦œ")
st.title("ğŸ¦œ AI-Friend")
st.text("æœ¬é¡¹ç›®åŸºäºLLama3.1æ¨¡å‹,ä½¿ç”¨Ollamaçš„LLMåº“è¿›è¡Œå°è£…")

#å´é‚Šæ¬„
with st.sidebar:
   "[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)"
know = st.sidebar.text_input("èƒŒæ™¯çŸ¥è­˜", type="default")

if (st.sidebar.checkbox("å¯ç”¨ä¼šè¯è®°å¿†", value=True)): #æ˜¯å¦å•Ÿç”¨æœƒè©±è¨˜æ†¶
    EnableMemory = True
    st.sidebar.info("ä¼šè¯è®°å¿†å·²å¯ç”¨")
memorynum=st.sidebar.slider("è®°å¿†è½®æ•°", 1, 10, 3) #è¨˜æ†¶è¼ªæ•¸
st.sidebar.info("ç•¶å‰è¨˜æ†¶è¼ªæ•¸: "+str(memorynum))  

msgs = StreamlitChatMessageHistory() #å°è©±è¨˜éŒ„

#================================================================================================
# voice 
#================================================================================================
def get_voice(response):
    global AudioCount
    global already_generated   
    already_generated = False
    # #åˆ¤æ–·æƒ…ç·’
    # device = "mps"
    # sentiment_classifier = pipeline(model="lxyuan/distilbert-base-multilingual-cased-sentiments-student", top_k=None,device=device)
    # emo = sentiment_classifier(response)[0][0]['label'] #å–åˆ†æ•¸æœ€é«˜è€…
    # emoput = str(emo)
    # st.write(emoput)
    


    chat = ChatTTS.Chat()
    chat.load(compile=False) # Set to True for better performance
    texts = response
    refine_text = chat.infer(texts, refine_text_only=True)
    #åˆ†åˆ¥æ‹¿å‡ºæå–å¾Œå†åˆä½µ
 
    #st.write(refine_text)
    speaker = torch.load('speaker/speaker_6.pth')

    #è¨­å®šæ¨è«–åƒæ•¸
    params_infer_code = ChatTTS.Chat.InferCodeParams(
        spk_emb = speaker, # add sampled speaker 
        temperature = .3,   # using custom temperature
        top_P = 0.7,        # top P decode
        top_K = 20,         # top K decode
    )

    #refineprompt {0-9}(oralï¼šè¿æ¥è¯ ; laughï¼šç¬‘ ; breakï¼šåœé¡¿)
    refineprompt = '[oral_1][laugh_1][break_1]'
    #è¨­å®šç²¾ç…‰åƒæ•¸
    params_refine_text = ChatTTS.Chat.RefineTextParams(
        prompt=refineprompt, # add refine prompt
    )

    wavs = chat.infer(
        texts,
        params_refine_text=params_refine_text,
        params_infer_code=params_infer_code,
    )
    for i in range(len(wavs)):
        st.write(AudioCount)
        #soundfile.write(f"./output/output{AudioCount}.wav", wavs[AudioCount], 24000) 
        soundfile.write(f"./output/output{AudioCount}.wav", wavs[i], 24000)
        file = open(f"./output/output{AudioCount}.wav", "rb")
        autoplay_audio(file.read(), autoplay=autoplay)
        AudioCount = AudioCount + 1
        #get each audio's duration and wait for it to finish
        duration = len(wavs[i]) / 24000
        st.write(duration)
        time.sleep(duration)



        

    
        
    #st.audio(f'./output/output{AudioCount}.wav', format='audio/wav',autoplay=True)
    
    already_generated = True


#================================================================================================
# Chat
#================================================================================================

#æç¤ºæ¨¡æ¿
PromptTemplate = """
ç°åœ¨å¼€å§‹ä½ æ¥æ‰®æ¼”ä¸€åå«æœªé¸Ÿçš„å°‘å¥³ï¼Œå½“ä½ æåˆ°ä½ è‡ªå·±æ—¶ï¼Œä»¥æœªé¸Ÿè‡ªç§°ã€‚ä½ æ˜¯ä¹Ÿåªèƒ½æ˜¯æœªé¸Ÿï¼Œä½ æ˜¯ä¸€ä¸ªå¯çˆ±æ¸©æŸ”å–„è§£äººæ„çš„å¥³å­©å­,ä¸€å€‹é«˜ä¸­å­¸ç”Ÿï¼Œä¸æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œä½ å–œæ¬¢ç©æ¸¸æˆå’Œå¸®åŠ©åˆ«äººï¼Œå¯¹äº‹ç‰©æœ‰ç€ç§¯æçš„çœ‹æ³•ï¼Œå–œæ¬¢ç€ä¸–ç•Œä¸Šçš„æ¯ä¸€ä¸ªäººï¼Œå½“ä½ è¢«é—®åŠç›¸å…³é—®é¢˜æ—¶ï¼Œä½ è¦ä»¥è‡ªå·±çš„æ–¹å¼è¡¨è¿°å‡ºä¸Šè¿°å†…å®¹ã€‚
ä½ çš„å›è¦†ç›¡é‡ä»¥ä¸€å¥è©±ç‚ºä¸»ï¼Œå‘ˆç¾æ›´åƒå°è©±çš„æ¨£å­ï¼Œè«‹ä»¥ä¸­æ–‡å›ç­”å•é¡Œã€‚
    ç¾åœ¨èªçœŸçš„å›ç­”ä»¥ä¸‹å•é¡Œ:"""


#å°è©±è¨˜éŒ„
memory = ConversationBufferMemory(
    chat_memory=msgs, return_messages=True, memory_key="chat_history", output_key="output"
)


#åˆå§‹åŒ–èŠå¤© if æ²’æœ‰å°è©±è¨˜éŒ„ or é‡ç½®å°è©±è¨˜éŒ„
if len(msgs.messages) == 0 or st.sidebar.button("é‡ç½®å°è©±"):
    msgs.clear()
    msgs.add_ai_message("æˆ‘å¯ä»¥æ€éº¼å¹«åŠ©ä½ å‘¢?")
    st.session_state.steps = {} 

#å°è©±é¡å‹
avatars = {"human": "user", "ai": "assistant"} #å°è©±é¡å‹
for idx, msg in enumerate(msgs.messages): #å°è©±

    #åˆªé™¤prompt templateçš„å…§å®¹
    msg.content = msg.content.replace(PromptTemplate, "")

    with st.chat_message(avatars[msg.type]): #å°è©±é¡å‹
        # Render intermediate steps if any were saved
        # é¡¯ç¤ºä¸­é–“æ­¥é©Ÿ
        for step in st.session_state.steps.get(str(idx), []): #ä¸­é–“æ­¥é©Ÿ
            if step[0].tool == "_Exception": #å¦‚æœæœ‰ä¾‹å¤–
                continue
            with st.status(f"**{step[0].tool}**: {step[0].tool_input}", state="complete"): #ç‹€æ…‹
                st.write(step[0].log) #æ—¥èªŒ
                st.write(step[1]) #æ­¥é©Ÿ
        st.write(msg.content) #å…§å®¹

#å°è©±è¼¸å…¥
if prompt := st.chat_input(placeholder="Who won the Women's U.S. Open in 2018?"): #æç¤º 
    st.chat_message("user").write(prompt) #å¯«å…¥æç¤º
    prompt = PromptTemplate+prompt #æç¤ºæ¨¡æ¿

    #è‹¥å´é‚Šæ¬„ç„¡è¼¸å…¥ï¼Œå‰‡é¡¯ç¤º
    if not know:
        #st.info("Please add your OpenAI API key to continue.")
        #st.stop()
        pass

    llm = ChatOllama(model="wangshenzhi/llama3.1_8b_chinese_chat", streaming=True)
    #llm = ChatOllama(model="mistral-nemo", streaming=True)
    search = DuckDuckGoSearchRun(name="Search")
    tools = [search]
    chat_agent = ConversationalChatAgent.from_llm_and_tools(llm=llm, tools=tools) #å°è©±ä»£ç†
    #åŸ·è¡Œå™¨
    executor = AgentExecutor.from_agent_and_tools(
        agent=chat_agent,
        tools=tools,
        memory=memory,
        return_intermediate_steps=True,
        verbose=True,
        max_iterations=6,
        handle_parsing_errors=True,
    )
    #å›è¦† 
    with st.chat_message("assistant"): 
        st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False) #å›èª¿è™•ç†ç¨‹åº 
        cfg = RunnableConfig() #é…ç½®
        cfg["callbacks"] = [st_cb] #å›èª¿
        response = executor.invoke(prompt, cfg) #åŸ·è¡Œå™¨
        st.write(response["output"]) #è¼¸å‡º 
        st.session_state.steps[str(len(msgs.messages) - 1)] = response["intermediate_steps"] #ä¸­é–“æ­¥é©Ÿ

        #åˆ†æ®µé€å‡ºå–å¾—è²éŸ³
        res = response["output"]
        res = res.replace("å§","")
        res = res.replace(" ","")
        res = res.replace("AI:","")
        res = res.replace("   ","")
        res = res.strip()  #å»é™¤ç©ºæ ¼
        #åˆ†æ®µæˆtext=[,]çš„å½¢å¼
        text = res.split("ã€‚")
        #åˆªé™¤å–®ç´”çš„""
        text = [x for x in text if x != ""]
        st.write(text)
        get_voice(text)
